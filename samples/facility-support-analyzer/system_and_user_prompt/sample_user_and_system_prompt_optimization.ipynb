{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc6e0e5-e71f-4eed-adbb-98546efded15",
   "metadata": {},
   "source": [
    "### Setup AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8954cb65-5eb1-4e76-92bb-331be63f7e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Setup your AWS Access Key and Secret Key as environment variables.\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\"\n",
    "os.environ[\"AWS_SESSION_TOKEN\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d981604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Nova Model\n",
    "NOVA_MODEL_ID = \"us.amazon.nova-premier-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc38c9-7b03-4c7b-824c-dac92201aa15",
   "metadata": {},
   "source": [
    "### Dataset Adapter\n",
    "\n",
    "Initialize the Dataset Adapter that takes the input_columns and output_columns. We use the CSVDatasetAdapter to read a `.csv` file and adapt it to the standardized format. We also use the adapter to create train and test sets for our use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da047488-0220-4e49-b8d0-361c5afcdfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.input_adapters.dataset_adapter import CSVDatasetAdapter\n",
    "\n",
    "input_columns = {\"input\"}\n",
    "output_columns = {\"answer\"}\n",
    "\n",
    "dataset_adapter = CSVDatasetAdapter(input_columns, output_columns)\n",
    "\n",
    "# Adapt\n",
    "dataset_adapter.adapt(\"../data/FacilitySupportAnalyzer.csv\")\n",
    "\n",
    "train_set, test_set = dataset_adapter.split(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22305416-5912-429f-b47b-04592a0966da",
   "metadata": {},
   "source": [
    "### Prompt Adapter\n",
    "\n",
    "Initialize the Prompt Adapter for the Original Prompt. For this example, we use the FacilitySupportAnalyzer System and User Prompt in the `.txt` format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621dc01d-6896-4a37-ad64-63f27c597f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.input_adapters.prompt_adapter import TextPromptAdapter\n",
    "\n",
    "prompt_variables = input_columns\n",
    "\n",
    "prompt_adapter = TextPromptAdapter()\n",
    "\n",
    "prompt_adapter.set_system_prompt(file_path=\"original_prompt/system_prompt.txt\", variables=prompt_variables)\n",
    "prompt_adapter.set_user_prompt(file_path=\"original_prompt/user_prompt.txt\", variables=prompt_variables)\n",
    "\n",
    "# Adapt\n",
    "prompt_adapter.adapt()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8349aeee-4f9a-49e0-94cb-029f2c4fb05f",
   "metadata": {},
   "source": [
    "### Metric Adapter\n",
    "\n",
    "Initialize the Metric Adapter for evaluating this prompt for certain optimizers. For this example, we build a Custom Metric for the FacilitySupportAnalyzer Dataset. The metric adapter requires the use of the `apply` [For single row evaluation] or `batch_apply` [For evaluating the whole dataset together] function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f093a481-31a0-4871-99a0-670a60d67b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.input_adapters.metric_adapter import MetricAdapter\n",
    "from typing import List, Any, Dict\n",
    "import re\n",
    "import json\n",
    "\n",
    "class FacilitySupportAnalyzerMetric(MetricAdapter):\n",
    "    def parse_json(self, input_string: str):\n",
    "        \"\"\"\n",
    "        Attempts to parse the given string as JSON. If direct parsing fails,\n",
    "        it tries to extract a JSON snippet from code blocks formatted as:\n",
    "            ```json\n",
    "            ... JSON content ...\n",
    "            ```\n",
    "        or any code block delimited by triple backticks and then parses that content. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            return json.loads(input_string)\n",
    "        except json.JSONDecodeError as err:\n",
    "            error = err\n",
    "\n",
    "        patterns = [\n",
    "            re.compile(r\"```json\\s*(.*?)\\s*```\", re.DOTALL | re.IGNORECASE),\n",
    "            re.compile(r\"```(.*?)```\", re.DOTALL)\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = pattern.search(input_string)\n",
    "            if match:\n",
    "                json_candidate = match.group(1).strip()\n",
    "                try:\n",
    "                    return json.loads(json_candidate)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "\n",
    "        raise error\n",
    "\n",
    "    def _calculate_metrics(self, y_pred: Any, y_true: Any) -> Dict:\n",
    "        strict_json = False\n",
    "        result = {\n",
    "            \"is_valid_json\": False,\n",
    "            \"correct_categories\": 0.0,\n",
    "            \"correct_sentiment\": False,\n",
    "            \"correct_urgency\": False,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            y_true = y_true if isinstance(y_true, dict) else (json.loads(y_true) if strict_json else self.parse_json(y_true))\n",
    "            y_pred = y_pred if isinstance(y_pred, dict) else (json.loads(y_pred) if strict_json else self.parse_json(y_pred))\n",
    "        except json.JSONDecodeError:\n",
    "            result[\"total\"] = 0\n",
    "            return result  # Return result with is_valid_json = False\n",
    "        else:\n",
    "            if isinstance(y_pred, str):\n",
    "                result[\"total\"] = 0\n",
    "                return result  # Return result with is_valid_json = False\n",
    "            result[\"is_valid_json\"] = True\n",
    "\n",
    "            categories_true = y_true.get(\"categories\", {})\n",
    "            categories_pred = y_pred.get(\"categories\", {})\n",
    "\n",
    "            if isinstance(categories_true, dict) and isinstance(categories_pred, dict):\n",
    "                correct = sum(\n",
    "                    categories_true.get(k, False) == categories_pred.get(k, False)\n",
    "                    for k in categories_true\n",
    "                )\n",
    "                result[\"correct_categories\"] = correct / len(categories_true) if categories_true else 0.0\n",
    "            else:\n",
    "                result[\"correct_categories\"] = 0.0  # or raise an error if you prefer\n",
    "\n",
    "            result[\"correct_sentiment\"] = y_pred.get(\"sentiment\", \"\") == y_true.get(\"sentiment\", \"\")\n",
    "            result[\"correct_urgency\"] = y_pred.get(\"urgency\", \"\") == y_true.get(\"urgency\", \"\")\n",
    "\n",
    "        # Compute overall metric score\n",
    "        result[\"total\"] = sum(\n",
    "            float(result[k]) for k in [\"correct_categories\", \"correct_sentiment\", \"correct_urgency\"]\n",
    "        ) / 3.0\n",
    "\n",
    "        return result\n",
    "\n",
    "    def apply(self, y_pred: Any, y_true: Any):\n",
    "        return self._calculate_metrics(y_pred, y_true)\n",
    "\n",
    "    def batch_apply(self, y_preds: List[Any], y_trues: List[Any]):\n",
    "        evals = [self.apply(y_pred, y_true) for y_pred, y_true in zip(y_preds, y_trues)]\n",
    "        float_keys = [k for k, v in evals[0].items() if isinstance(v, (int, float, bool))]\n",
    "        return {k: sum(e[k] for e in evals) / len(evals) for k in float_keys}\n",
    "\n",
    "metric_adapter = FacilitySupportAnalyzerMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852c904-42e8-46a2-ad9e-a2e88d8eba6a",
   "metadata": {},
   "source": [
    "### Inference Adapter\n",
    "Initialize the InferenceAdapter to choose the backend Inference. Currently, we only support BedrockInferenceAdapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea654ddd-59d0-495e-8e56-29fe0ed6dd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.inference.adapter import BedrockInferenceAdapter\n",
    "\n",
    "inference_adapter = BedrockInferenceAdapter(region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb20883-793c-4b38-8e2c-7f2d461cc2fb",
   "metadata": {},
   "source": [
    "### Evaluator\n",
    "\n",
    "The Evaluator can use the metric_adapter, prompt_adapter, and dataset_adapter to evaluate the prompt given the `model_id` to produce an evaluation score. The Evaluator internally uses the `InferenceRunner` to first generate inference results and then evaluate the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb5ac6",
   "metadata": {},
   "source": [
    "#### Base Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f158430-ed74-470c-8657-2b49b57ae79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.evaluation import Evaluator\n",
    "\n",
    "evaluator = Evaluator(prompt_adapter, test_set, metric_adapter, inference_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33974a07-c556-4238-a39c-12601fa01e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_prompt_score = evaluator.aggregate_score(model_id=NOVA_MODEL_ID)\n",
    "\n",
    "print(f\"Original Prompt Evaluation Score = {original_prompt_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab372d4-8ffc-4104-a5f6-30160a48f9f6",
   "metadata": {},
   "source": [
    "### Optimization Adapter\n",
    "\n",
    "We can now define the Optimization Functions. The Optimization function takes as input the Prompt Adapter and Optionally a Dataset Adapter, Inference Adapter, and Metric Adapter. The optimization function optimizes the prompt and returns a Prompt Adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8241fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacilitySupportAnalyzerNovaMetric(FacilitySupportAnalyzerMetric):\n",
    "    def apply(self, y_pred: Any, y_true: Any):\n",
    "        # Requires to return a value and not a JSON payload\n",
    "        return self._calculate_metrics(y_pred, y_true)[\"total\"]\n",
    "        \n",
    "    def batch_apply(self, y_preds: List[Any], y_trues: List[Any]):\n",
    "        pass\n",
    "nova_metric_adapter = FacilitySupportAnalyzerNovaMetric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9f98bc",
   "metadata": {},
   "source": [
    "#### NovaPromptOptimizer\n",
    "\n",
    "NovaPromptOptimizer = Nova Meta Prompter + MIPROv2 with Nova Model Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c2d973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.optimizers import NovaPromptOptimizer\n",
    "\n",
    "nova_prompt_optimizer = NovaPromptOptimizer(prompt_adapter=prompt_adapter, inference_adapter=inference_adapter, dataset_adapter=train_set, metric_adapter=nova_metric_adapter)\n",
    "\n",
    "optimized_prompt_adapter = nova_prompt_optimizer.optimize(mode=\"pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1de0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_prompt_adapter.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a051afb",
   "metadata": {},
   "source": [
    "### Optimized System Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b52d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimized_prompt_adapter.system_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90efbb24",
   "metadata": {},
   "source": [
    "### Optimized User Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795751a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimized_prompt_adapter.user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cb01f0-7d07-4bd2-ab0f-4dced7d55da7",
   "metadata": {},
   "source": [
    "### Few Shot Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa1fdb-7273-4c76-a0ad-e8a0692bfbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of Few-Shot Examples = {len(optimized_prompt_adapter.few_shot_examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e3b16-0f49-41cd-a68b-d6ccbc51b656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only the first example\n",
    "print(optimized_prompt_adapter.few_shot_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d30a347",
   "metadata": {},
   "source": [
    "### Evaluator\n",
    "\n",
    "Now we evaluate the Nova Prompt Optimizer Optimized prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65493b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from amzn_nova_prompt_optimizer.core.evaluation import Evaluator\n",
    "\n",
    "evaluator = Evaluator(optimized_prompt_adapter, test_set, metric_adapter, inference_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nova_prompt_optimizer_evaluation_score = evaluator.aggregate_score(model_id=NOVA_MODEL_ID)\n",
    "print(f\"Nova Prompt Optimizer = {nova_prompt_optimizer_evaluation_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98a74bbb-1744-4daa-a17e-89ede77fee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_prompt_adapter.save(\"optimized_prompt/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
